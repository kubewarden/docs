include::partial$variables.adoc[]
= {project-name} vs OPA Gatekeeper
:revdate: 2025-11-03
:page-revdate: {revdate}
:page-aliases: explanations/opa-comparison.adoc
:description: A brief comparison of the difference between {project-name} and OPA Gatekeeper.
:doc-persona: ["kubewarden-all"]
:doc-topic: ["explanations", "kubewarden-vs-opa_gatekeeper"]
:doc-type: ["explanation"]
:keywords: ["{project-name}", "kubewarden", "kubernetes", "opa gatekeeper", "comparison"]
:current-version: {page-origin-branch}

[IMPORTANT]
====

This page is from August 2023. Both projects may have evolved since then.

If you find something is missing or inaccurate, please
https://github.com/kubewarden/docs/[file an issue] or open a PR using the link
at the bottom of the page.

====

Both OPA Gatekeeper and Kubewarden, from which {project-name} is derived, are open source projects, 
and part of CNCF.

This table provides a comparison between OPA Gatekeeper and {short-project-name}. Topics
requiring more information have links to further explanation.

|===
| Feature | OPA Gatekeeper | {short-project-name}

| Validation
| ✅
| ✅

| Mutation
| ✅
| ✅

| Policy language <<_writing_policies,[1]>>
| Rego
| Rego, CEL, Go, Rust,...

| Context aware <<_context_aware,[2]>>
| ✅
| ✅

| Kubernetes integration <<_kubernetes_integration,[3]>>
| cluster wide CRD
| cluster wide and namespaced CRDs

| Policy distribution <<_policy_distribution,[4]>>
| embedded into Kubernetes CR
| Container registry, or embedded into Kubernetes CR (CEL)

| CI/CD integration <<_cicd_integration,[5]>>
| ✅
| ✅

| Policy enforcement modes
| deny, warn
| deny, warn

| Deployment mode <<_deployment_mode,[6]>>
| single evaluation server
| multiple evaluation servers

| Background checks <<_background_checks,[7]>>
| ✅
| ✅
|===

== Types of policies

Both OPA Gatekeeper and Kubernetes can write validation and mutation policies.

These policies can target any kind of Kubernetes resource, including Custom
Resources.

== Writing policies

You write OPA Gatekeeper policies using
https://www.openpolicyagent.org/docs/latest/#rego[Rego]. Rego is a query
language created by the Open Policy Agent project.

[IMPORTANT]
====

You can only use Rego to write validating policies. Mutating policies don't
use Rego, instead using ad-hoc rules defined in YAML (see
https://open-policy-agent.github.io/gatekeeper/website/docs/mutation[here]).

====


You can write {short-project-name} policies using different paradigms. Policy authors can
use both traditional programming languages (like Go, Rust and others) or
https://en.wikipedia.org/wiki/Domain-specific_language[Domain Specific
Languages] like Rego and CEL. In {short-project-name}'s you write validating and mutating
policies the same way.

[CAUTION]
====

The https://github.com/open-policy-agent/kube-mgmt[kube-mgmt]
open source project, part of the Open Policy Agent project, uses Rego.

Despite using the same language, policies written for kube-mgmt aren't
compatible with OPA Gatekeeper and vice versa.

{short-project-name} can use Rego policies written for both Open Policy
Agent and for OPA Gatekeeper. More information is
xref:/tutorials/writing-policies/rego/01-intro-rego.adoc[here].

====


== Context aware

Sometimes a policy needs data about the current state of the cluster to make a
validation or mutation decision. For example, a policy validating Ingress
resources might need to know other Ingress resources already defined in the
cluster to ensure no clashes happen. {short-project-name} calls these policies "context
aware".

Both OPA Gatekeeper and {short-project-name} support these types of policies.

When deploying OPA Gatekeeper, a Kubernetes administrator decides which type of
cluster data to make available to the policies at evaluation time.

It's important to highlight how this data is then accessible by all the
policies deployed.

For example, if an OPA Gatekeeper policy requires access to Kubernetes Secrets,
all the other policies deployed are able to read this data as well.

Conversely, {short-project-name} provides a granular access to cluster resources.
Each policy has access only to the resources that the Kubernetes administrator
specifies. Attempting to read unauthorized data is immediately blocked and
reported to the cluster administrators.

== Kubernetes integration

OPA Gatekeeper has a cluster wide Custom Resource, permitting policy
definition, and how and where to enforce them.

{short-project-name} has two different types of Custom Resources used to declare
policies. One works at the Cluster level, the other is namespaced. The name of
the namespaced Custom Resource is `AdmissionPolicy`.

Policies deployed via a `AdmissionPolicy` resource affect only the resources
created within the namespace they belong to. Because of that, you can allow
non-administration Kubernetes users to have the RBAC privileges necessary to
manage `AdmissionPolicy` resources, in the namespaces they can access.

This allows Kubernetes administrators to delegate policy-related work.

== Policy distribution

OPA Gatekeeper and {short-project-name} policies have policy source code (Rego for OPA
Gatekeeper, CEL for {short-project-name}) in the Custom Resource defining a policy in
Kubernetes.

Also, {short-project-name} policies can have the source code of the policy managed like
container images (for Rego, Go, Rust, ...). Once built, they're pushed into
container registries as OCI artifacts.

You can sign and verify {short-project-name} policies using container image tools like
`cosign`, from the https://sigstore.dev[Sigstore project].

You can distribute {short-project-name} policies among geographically distributed
container registries using the traditional tools and processes adopted for
container images.

== CI/CD integration

Both OPA Gatekeeper and {short-project-name} are managed using GitOps methodologies.

However, for OPA Gatekeeper, there's a coupling between the policy's Rego code
and the Custom Resource used to deploy it in Kubernetes.
This introduces extra steps in CI/CD pipelines.

Rego has https://www.openpolicyagent.org/docs/latest/policy-testing/[testing
tools] allowing the creation of unit test suites. Writing tests and executing
them in a CI/CD pipeline is essential to ensure policies behave as expected.

To use these testing tools, the source code of the policy must be available in
dedicated text files. It's impossible to read the source code from the YAML
files used to declare the OPA Gatekeeper policy. The CI/CD pipeline must
synchronize the Rego source code to test with the code defined in the OPA
Gatekeeper Custom Resource. You can do this using third party tools.

{short-project-name} policies have CI/CD pipelines like traditional microservices.
Usually their source code lives in a Git repository and then, using traditional
CI/CD systems, unit tests run against it. You write unit tests using the
testing frameworks of the language used to write the policy. Once all the tests
pass, you compile the policy to WebAssembly and push it to a container
registry. This kind of pipeline is usually maintained by the policy author.

Kubernetes administrators typically maintain other automation pipelines that react to
new releases of the policy (using automation tools like
https://docs.github.com/en/code-security/dependabot/working-with-dependabot[Dependabot],
https://www.mend.io/renovate/[Renovate bot],
https://www.updatecli.io/[updatecli] and others), or to changes to the
policy configuration.

The pipeline tests the policy against different types of requests. You can do
the testing using the https://github.com/kubewarden/kwctl[`kwctl`] CLI tool,
without requiring a running Kubernetes cluster. The `kwctl` CLI tool uses the
same evaluation engine used by the {short-project-name} stack deployed in a Kubernetes
cluster.

== Policy enforcement modes

Both OPA Gatekeeper and {short-project-name} can deploy policies using two different
operation modes:

* `deny`: violation of a policy rejects the request
* `warn`: violation of a policy doesn't cause rejection and is logged

== Deployment mode

The same server evaluates all the OPA Gatekeeper policies. Conversely,
{short-project-name} allows definition of multiple evaluation servers. You define these
servers by a Custom Resource called `PolicyServer`.

When declaring a {short-project-name} policy, the Kubernetes administrator decides which
`PolicyServer` host it.

[NOTE]
====

The `PolicyServer` object is a high level abstraction introduced by {short-project-name}.
Behind the scenes a `Deployment` with a specific replica size is created.

Each `PolicyServer` can have a different replica size from others.

====

This allows interesting scenarios like the following ones:

* Deploy critical policies to a dedicated Policy Server pool.
* Deploy the policies of a noisy tenant to a dedicated Policy Server pool.

== Background checks

As policies are added, removed, and reconfigured the resources already in the
cluster might become non-compliant.

Both OPA Gatekeeper and {short-project-name} have a scanner that operates in the
background. This scanner evaluates resources already defined in the cluster and
flags non-compliant ones.

The only difference between OPA Gatekeeper and {short-project-name} is how the scanner
results get saved.

OPA Gatekeeper adds the violation details to the `status` field of a given
`Constraint` Custom Resource (see
https://open-policy-agent.github.io/gatekeeper/website/docs/audit#constraint-status[here]).

{short-project-name} instead stores the results inside of a set of the Reports
Custom Resources defined by https://openreports.io[openreports.io].
