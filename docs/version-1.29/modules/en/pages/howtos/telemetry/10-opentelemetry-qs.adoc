include::partial$variables.adoc[]
= Open Telemetry quick start
:revdate: 2025-07-09
:page-revdate: {revdate}
:description: An Open Telemetry quickstart for {short-project-name}.
:doc-persona: ["kubewarden-operator", "kubewarden-integrator"]
:doc-topic: ["operator-manual", "telemetry", "opentelemetry", "quick-start"]
:doc-type: ["howto"]
:keywords: ["kubewarden", "kubernetes", "opentelemetry", "open telemetry", "quickstart"]
:sidebar_label: Open Telemetry
:current-version: {page-origin-branch}

https://opentelemetry.io/[OpenTelemetry] is a Cloud Native Computing Foundation
(CNCF) framework for observability. It enables your microservices to provide
metrics, logs and traces.

{short-project-name}'s components, using the OpenTelemetry SDK, report data to
an OpenTelemetry collector â€” called the agent.

This guide explains how to deploy the OpenTelemetry collector in `sidecar` mode
by using the official Kubernetes Helm chart.

This is a simple deployment pattern using OpenTelemetry. It's final setup looks
like this:

* Each Pod of the {short-project-name} stack (Policy Server, Controller) has an
  OpenTelemetry sidecar.
* The sidecar receives tracing and monitoring information from the
  {short-project-name} component via the OpenTelemetry Protocol (OTLP)
* The OpenTelemetry collector:
 ** Sends the trace events to a central Jaeger instance
 ** Exposes Prometheus metrics on a specific port

The {short-project-name} Helm chart doesn't cover all the possible deployment
scenarios of the OpenTelemetry collector. It's also possible to configure
{short-project-name} to send data to an OpenTelemetry collector. Documentation
for that scenario is in the xref:./40-custom-otel-collector.adoc[custom
OpenTelemetry guide].

You first deploy OpenTelemetry in a Kubernetes cluster, so you can use it in
the following sections addressing specifically tracing and metrics.

== Setting up a Kubernetes cluster

____
This section has step-by-step instructions for creating a
Kubernetes cluster with an ingress controller enabled.

Feel free to skip this section if you already have a Kubernetes
cluster where you can define Ingress resources.
____

You can create a Kubernetes cluster for testing using https://minikube.sigs.k8s.io/docs/[minikube].

Minikube has many backends, for this case you can use the
https://minikube.sigs.k8s.io/docs/drivers/kvm2/[kvm2] driver
which relies on libvirt.

Assuming `libvirtd` is correctly running on your machine, issue the
following command:

[subs="+attributes",console]
----
minikube start --driver=kvm2
----

The command produces an output similar to the following:

[subs="+attributes",console]
----
$ minikube start --driver=kvm2
ğŸ˜„  minikube v1.23.2 on Opensuse-Leap 15.3
âœ¨  Using the kvm2 driver based on user configuration
ğŸ‘  Starting control plane node minikube in cluster minikube
ğŸ”¥  Creating kvm2 VM (CPUs=2, Memory=6000MB, Disk=20000MB) ...
ğŸ³  Preparing Kubernetes v1.22.2 on Docker 20.10.8 ...
    â–ª Generating certificates and keys ...
    â–ª Booting up control plane ...
    â–ª Configuring RBAC rules ...
ğŸ”  Verifying Kubernetes components...
    â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
ğŸŒŸ  Enabled addons: storage-provisioner, default-storageclass
ğŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default
----

Now you need to enable the Ingress add-on:

[subs="+attributes",console]
----
minikube addons enable ingress
----

This produces an output similar to the following one:

[subs="+attributes",console]
----
$ minikube addons enable ingress
    â–ª Using image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.0
    â–ª Using image registry.k8s.io/ingress-nginx/controller:v1.0.0-beta.3
    â–ª Using image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.0
ğŸ”  Verifying ingress addon...
ğŸŒŸ  The 'ingress' addon is enabled
----

[#install-opentelemetry]
== Install OpenTelemetry

You use the
https://github.com/open-telemetry/opentelemetry-operator[OpenTelemetry
Operator] to manage the automatic injection of the OpenTelemetry Collector
sidecar into the PolicyServer pod.

The OpenTelemetry Operator requires installation of
https://cert-manager.io/docs/installation/[cert-manager] in the cluster.

At the time of writing (2022-06-21), only specific versions of OpenTelemetry
are compatible with Cert Manager,
https://github.com/open-telemetry/opentelemetry-operator#opentelemetry-operator-vs-kubernetes-vs-cert-manager[see
the compat chart].

You should install the latest cert-manager Helm chart:

[NOTE]
====
At the time of writing (2024-07-17) the latest cert-manager chart version is `v1.15.1`
====


[subs="+attributes",console]
----
helm repo add jetstack https://charts.jetstack.io

helm install --wait \
    --namespace cert-manager \
    --create-namespace \
    --set crds.enabled=true \
    --version 1.15.1 \
    cert-manager jetstack/cert-manager
----

Once cert-manager is running, you can install the OpenTelemetry operator Helm
chart:

[NOTE]
====

At the time of writing (2024-11-11) the latest OpenTelemetry operator chart
version is `0.65.0`

====

[subs="+attributes",console]
----
helm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts

helm install --wait \
  --namespace open-telemetry \
  --create-namespace \
  --version 0.65.0 \
  --set "manager.collectorImage.repository=otel/opentelemetry-collector-contrib" \
  my-opentelemetry-operator open-telemetry/opentelemetry-operator
----

== OpenTelemetry integration

You can now move to the next chapters to enable application metrics (via
Prometheus integration) and application tracing (via Jaeger integration).
